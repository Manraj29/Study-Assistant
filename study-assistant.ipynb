{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yXzm_PkBsxJD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8fcb28bb14d842c195ede5e3103e7de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "English",
              "Hindi",
              "Marathi",
              "Bengali",
              "Gujarati",
              "Tamil",
              "Telugu",
              "Kannada",
              "Malayalam",
              "Punjabi",
              "Odia",
              "Assamese",
              "Nepali",
              "Urdu",
              "Sindhi",
              "Bhojpuri",
              "Manipuri",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Japanese",
              "Korean",
              "Russian"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select Input Language:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_09e9042bb5d1489ab400e6925f0df44a",
            "style": "IPY_MODEL_abd092f410c04ffdbc709dc90eed74bb"
          }
        },
        "09e9042bb5d1489ab400e6925f0df44a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abd092f410c04ffdbc709dc90eed74bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f988c65b2e1f4859a9e95e3089b0cb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "English",
              "Hindi",
              "Marathi",
              "Bengali",
              "Gujarati",
              "Tamil",
              "Telugu",
              "Kannada",
              "Malayalam",
              "Punjabi",
              "Odia",
              "Assamese",
              "Nepali",
              "Urdu",
              "Sindhi",
              "Bhojpuri",
              "Manipuri",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Japanese",
              "Korean",
              "Russian"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select Output Language:",
            "description_tooltip": null,
            "disabled": false,
            "index": 14,
            "layout": "IPY_MODEL_f48b396535db46c1994cdf2439d7a157",
            "style": "IPY_MODEL_810449c393f64e0ea875d7fda078c0e7"
          }
        },
        "f48b396535db46c1994cdf2439d7a157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "810449c393f64e0ea875d7fda078c0e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Install all the Libraries\n",
        "!pip install googletrans==3.1.0a0\n",
        "!pip install pymupdf\n",
        "!pip install -q transformers==4.43.1 einops accelerate langchain bitsandbytes sentencepiece langchain_community langchain_huggingface\n",
        "!pip install spacy\n",
        "!pip install Together\n",
        "!pip install deep-translator\n",
        "!pip install -q transformers==4.43.1 einops accelerate langchain bitsandbytes sentencepiece langchain_community langchain_huggingface\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "I-wrbd9PXue-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d861eb2-ff9a-49fe-f3b6-77f41b5e65d2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.10/dist-packages (3.1.0a0)\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Using cached httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.8.30)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.10.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Using cached httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Using cached h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Using cached httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "Using cached httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "Using cached h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "Installing collected packages: h11, httpcore, httpx\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.6\n",
            "    Uninstalling httpcore-1.0.6:\n",
            "      Successfully uninstalled httpcore-1.0.6\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.2\n",
            "    Uninstalling httpx-0.27.2:\n",
            "      Successfully uninstalled httpx-0.27.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.1.133 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.9.0 httpcore-0.9.1 httpx-0.13.3\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.11)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "googletrans 3.1.0a0 requires httpx==0.13.3, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: Together in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /usr/local/lib/python3.10/dist-packages (from Together) (3.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from Together) (8.1.7)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from Together) (0.2.0)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.10/dist-packages (from Together) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from Together) (1.26.4)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from Together) (10.4.0)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from Together) (16.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from Together) (2.9.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from Together) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /usr/local/lib/python3.10/dist-packages (from Together) (13.9.1)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from Together) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.10/dist-packages (from Together) (4.66.5)\n",
            "Requirement already satisfied: typer<0.13,>=0.9 in /usr/local/lib/python3.10/dist-packages (from Together) (0.12.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->Together) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->Together) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->Together) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->Together) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->Together) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->Together) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->Together) (4.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->Together) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->Together) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->Together) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->Together) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->Together) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->Together) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->Together) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.8.1->Together) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.8.1->Together) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13,>=0.9->Together) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->Together) (0.1.2)\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.8.30)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Implementation"
      ],
      "metadata": {
        "id": "4V2ZLudls5Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load the API Key and Model\n",
        "import os\n",
        "from together import Together\n",
        "from deep_translator import GoogleTranslator\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import spacy\n",
        "import fitz\n",
        "import time\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Markdown\n",
        "\n",
        "Lkey = userdata.get('Llama_Key')\n",
        "client = Together(api_key=Lkey)\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\""
      ],
      "metadata": {
        "id": "jFUSyXXnVylW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title All the functions\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "def generate_study_plan(days, syllabus):\n",
        "    input_prompt = f\"Create a detailed roadmap to complete the following syllabus in {days} days:\\n{syllabus}, not before the days i specified. It should cover {days}\"\n",
        "    prompt = \"\"\"\n",
        "    You are an expert in making study plans. You are an academic assistant who helps users plan their studies.\n",
        "    You have to make a detailed study plan. Share the study resources for each topic, the number of focused hours to study.\n",
        "    Dont give me any outline for the study plan, I need the detailed brief study plan with covering the important topics first and taking proper time to cover them.\n",
        "    Cover the Important topics from the syllabus first and then cover rest of the things.\n",
        "    Dont respond with anything else other than the study plan. When you tell the day along with the day also tell what should be done for the day.\n",
        "    Share some important resources for each topic like youtube videos with Youtube video title, books, etc. Dont share if the resources or youtube videos are wrong, if wrong then try sharing the topic name that they can search and get better results.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": input_prompt},\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=4096,\n",
        "        temperature=0.6,\n",
        "        top_p=0.7,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1,\n",
        "        stop=[\"<|eot_id|>\",\"<|eom_id|>\"],\n",
        "        stream=False\n",
        "    )\n",
        "    return (response.choices[0].message.content)\n",
        "\n",
        "# Translate the plan to user's preferred language\n",
        "def translate_study_plan(plan_text, target_lang):\n",
        "    translator = GoogleTranslator(source='auto', target=target_lang)\n",
        "    translated_chunks = []\n",
        "    # if target_lang == 'en':\n",
        "    #       chunks = split_text(plan_text, max_length=5000)\n",
        "    #       # Translate each chunk and store the result\n",
        "    #       for chunk in chunks:\n",
        "    #           translated_chunks.append(translator.translate(chunk))\n",
        "    #       # Join the translated chunks back into a single string\n",
        "    #       translated_plan = \" \".join(translated_chunks)\n",
        "    #       return translated_plan\n",
        "\n",
        "    chunks = split_text(plan_text, max_length=1000)  # Try smaller chunks\n",
        "    for chunk in chunks:\n",
        "        attempts = 3\n",
        "        for attempt in range(attempts):\n",
        "            try:\n",
        "                translated_chunks.append(translator.translate(chunk))\n",
        "                time.sleep(1)  # Short sleep to avoid rate limit\n",
        "                break  # Exit loop on success\n",
        "            except RequestError as e:\n",
        "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "        else:\n",
        "            print(\"All attempts failed for chunk:\", chunk)  # Log failure\n",
        "\n",
        "    # Join the translated chunks back into a single string\n",
        "    translated_plan = \" \".join(translated_chunks)\n",
        "    return translated_plan\n",
        "\n",
        "# Read PDF content using PyMuPDF\n",
        "def read_pdf_input(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)  # Load each page\n",
        "        text += page.get_text(\"text\")  # Extract text from page\n",
        "    return text\n",
        "\n",
        "\n",
        "def read_text_input(text_path):\n",
        "    if os.path.exists(text_path):\n",
        "        with open(text_path, 'r') as file:\n",
        "            return file.read()\n",
        "    return text_path\n",
        "\n",
        "\n",
        "def extract_topics(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    topics = set()\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"ORG\", \"GPE\", \"PERSON\", \"WORK_OF_ART\", \"EVENT\"}:\n",
        "            topics.add(ent.text)\n",
        "    return list(topics)\n",
        "\n",
        "def generate_summary(syllabus):\n",
        "    input_prompt = f\"Generate a simple summary for this syllabus: {syllabus}\"\n",
        "    prompt = \"\"\"\n",
        "    You are an expert in genearting summaries and getting the important topics for the user syllabus.\n",
        "    Be friendly with the user, suggest the summary with proper points.\n",
        "    Dont respond with anything else than the summary.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": input_prompt},\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=512,\n",
        "        temperature=0.6,\n",
        "        top_p=0.7,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1,\n",
        "        stop=[\"<|eot_id|>\",\"<|eom_id|>\"],\n",
        "        stream=False\n",
        "    )\n",
        "    return (response.choices[0].message.content)\n",
        "\n",
        "def split_text(text, max_length):\n",
        "    chunks = []\n",
        "    # print(\"slippting\")\n",
        "    while len(text) > max_length:\n",
        "        # Find the last space within the max_length to avoid splitting words\n",
        "        split_point = text[:max_length].rfind(' ')\n",
        "        chunks.append(text[:split_point])\n",
        "        text = text[split_point:]\n",
        "    chunks.append(text)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "JstgYC5W0jqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "def get_languages():\n",
        "    languages = {\n",
        "      'English': 'en',\n",
        "      'Hindi': 'hi',\n",
        "      'Marathi': 'mr',\n",
        "      'Bengali': 'bn',\n",
        "      'Gujarati': 'gu',\n",
        "      'Tamil': 'ta',\n",
        "      'Telugu': 'te',\n",
        "      'Kannada': 'kn',\n",
        "      'Malayalam': 'ml',\n",
        "      'Punjabi': 'pa',\n",
        "      'Odia': 'or',\n",
        "      'Assamese': 'as',\n",
        "      'Nepali': 'ne',\n",
        "      'Urdu': 'ur',\n",
        "      'Sindhi': 'sd',\n",
        "      'Bhojpuri': 'bho',\n",
        "      'Manipuri': 'mni',\n",
        "      'Spanish': 'es',\n",
        "      'French': 'fr',\n",
        "      'German': 'de',\n",
        "      'Italian': 'it',\n",
        "      'Japanese': 'ja',\n",
        "      'Korean': 'ko',\n",
        "      'Russian': 'ru',\n",
        "    }\n",
        "\n",
        "    # Create a dropdown for input language selection\n",
        "    input_langauge = widgets.Dropdown(\n",
        "        options=languages.keys(),\n",
        "        description='Select Input Language:',\n",
        "    )\n",
        "\n",
        "    # Create a dropdown for input language selection\n",
        "    output_langauge = widgets.Dropdown(\n",
        "        options=languages.keys(),\n",
        "        description='Select Output Language:',\n",
        "    )\n",
        "\n",
        "    display(input_langauge)\n",
        "    display(output_langauge)\n",
        "    return languages, input_langauge, output_langauge\n",
        "\n",
        "def get_code(languages, input_langauge, output_langauge):\n",
        "    for key, value in languages.items():\n",
        "        if key == input_langauge.value:\n",
        "            input_lang = value\n",
        "        if key == output_langauge.value:\n",
        "            output_lang = value\n",
        "    return input_lang, output_lang\n"
      ],
      "metadata": {
        "id": "88LQxvWYujx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main working"
      ],
      "metadata": {
        "id": "2nloX_7ryXj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "languages, input_langauge, output_langauge = get_languages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79,
          "referenced_widgets": [
            "8fcb28bb14d842c195ede5e3103e7de5",
            "09e9042bb5d1489ab400e6925f0df44a",
            "abd092f410c04ffdbc709dc90eed74bb",
            "f988c65b2e1f4859a9e95e3089b0cb1f",
            "f48b396535db46c1994cdf2439d7a157",
            "810449c393f64e0ea875d7fda078c0e7"
          ]
        },
        "id": "-b5LdEU5xWDH",
        "outputId": "d150b122-e9b3-42ef-ca26-1b96849e8efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select Input Language:', options=('English', 'Hindi', 'Marathi', 'Bengali', 'Gujarati', …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fcb28bb14d842c195ede5e3103e7de5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select Output Language:', options=('English', 'Hindi', 'Marathi', 'Bengali', 'Gujarati',…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f988c65b2e1f4859a9e95e3089b0cb1f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_code, output_code = get_code(languages, input_langauge, output_langauge)\n",
        "print(input_code, \" to \", output_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CoVfpw2xitB",
        "outputId": "0191010c-614d-4951-d974-8d1484720d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en  to  sd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get syllabus input from the user\n",
        "# print(\"Enter syllabus (text or PDF path):\")\n",
        "print(translate_study_plan(\"Enter syllabus (text or PDF path):\", input_code))\n",
        "input_syllabus = input().strip()\n",
        "\n",
        "# Handle PDF input if provided\n",
        "syllabus = \"\"\n",
        "try:\n",
        "    if input_syllabus.endswith('.pdf'):\n",
        "        syllabus = read_pdf_input(input_syllabus)\n",
        "    else:\n",
        "        syllabus = read_text_input(input_syllabus)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    printmd(\"**Error:** The specified file was not found. Please check the path and try again.\")\n",
        "    raise  # Optional: re-raise the exception if you want to stop the program\n",
        "\n",
        "except Exception as e:\n",
        "    printmd(f\"**Error:** An unexpected error: {e}\")\n",
        "    raise  # Optional: re-raise the exception if you want to stop the program\n",
        "\n",
        "# Convert syllabus text to English\n",
        "if syllabus:\n",
        "    syllabus_text = translate_study_plan(syllabus, 'en')\n",
        "    printmd(\"---\")\n",
        "    printmd(\"### Syllabus Content\")\n",
        "    printmd(syllabus_text)\n",
        "    printmd(\"---\")\n",
        "\n",
        "    # Get the number of days to complete the syllabus\n",
        "    # print(\"Enter the number of days to complete the syllabus (or press Enter to skip):\")\n",
        "    print(translate_study_plan(\"Enter the number of days to complete the syllabus (or press Enter to skip):\", input_code))\n",
        "    days_input = input().strip()\n",
        "    days = int(days_input) if days_input else 7  # Default to 7 days\n",
        "\n",
        "    printmd(\"---\")\n",
        "    printmd(\"### Summary of the syllabus:\")\n",
        "    summary = generate_summary(syllabus_text)\n",
        "\n",
        "    # Translate the summary to the user's preferred language\n",
        "    translated_summ = translate_study_plan(summary, output_code)\n",
        "    printmd(translated_summ)\n",
        "    printmd(\"---\")\n",
        "\n",
        "    printmd(\"### Study Plan:\")\n",
        "    study_plan = generate_study_plan(days, syllabus_text)\n",
        "\n",
        "    # Output the translated study plan\n",
        "    translated_plan = translate_study_plan(study_plan, output_code)\n",
        "    printmd(translated_plan)\n",
        "    printmd(\"---\")\n",
        "\n",
        "    printmd(\"### Key Topics\")\n",
        "    array_topics = extract_topics(syllabus_text)\n",
        "    key_topics = \", \".join(array_topics)\n",
        "    # printmd(key_topics)\n",
        "\n",
        "    # Translate key topics to the user's preferred language\n",
        "    translated_topics = translate_study_plan(key_topics, output_code)\n",
        "    printmd(translated_topics)\n",
        "    printmd(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I1NUwRkk0g-q",
        "outputId": "58fb9b77-4a6a-42a0-b61b-8a3ae2311e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter syllabus (text or PDF path):\n",
            "/content/test.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Syllabus Content"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Module Contents \n  \n1.0 \nIntroduction  \nOrigin & History of NLP; Language, Knowledge and Grammar in language processing \n  \nStages in NLP; Ambiguities and its types in English and Indian Regional Languages; \nChallenges of NLP; Applications of NLP \n2.0 \nWord Level Analysis  \n2.1 Basic Terms: Tokenization, Stemming, Lemmatization; \nSurvey of English Morphology, Inflectional Morphology, Derivational Morphology; \n2.2 Morphological Models: Dictionary lookup, finite state morphology; \nMorphological parsing with FST (Finite State Transducer) \n2.3 Grams and its variation: Bigram, Trigram; Simple (Unsmoothed) N-grams; N-\ngram Sensitivity to the Training Corpus; Unknown Words: Open versus closed \nvocabulary tasks; Evaluating N-grams: Perplexity; Smoothing: \nLaplace Smoothing \n  \n3.0 \nSyntax analysis  \n 3.1 Part-Of-Speech tagging(POS); Tag set for English (Upenn Treebank); Difficulties \n/Challenges in POS tagging; Rule-based, Stochastic and Transformation-based \ntagging; \n3.2 Generative Model: Hidden Markov Model /HMM Viterbi for POS tagging; Issues \nin HMM POS tagging; Discriminative Model: Maximum Entropy model, Conditional \nrandom Field (CRF); Parsers \n  \n4.0 \nSemantic Analysis \n4.1 Introduction, meaning representation; Lexical Semantics; Corpus study; Study \nof Various language dictionaries like WorldNet \nRelations among lexemes & their senses –Homonymy, Polysemy, Synonymy, \nHyponymy;Semantic Ambiguity; Word Sense Disambiguation (WSD);Knowledge \nbased approach( Lesk‘s Algorithm) \n  \n5.0 \nPragmatics  \nDiscourse: Reference Resolution, Reference Phenomena, Syntactic & Semantic \nconstraint on coherence, Anaphora  \n  \n6.0 \nGenerative AI  , Prompt Engineering and Large Language Models \n \n6.1 Introduction to Generative AI , Types of Generative AI Models (Variational Auto \nEncoders, Generative Adversarial Networks), Advantages and limitations of \nGenerative AI,ChatGPT. \n6.2 Prompt Engineering prompts for LLM interaction, Prompt Templates,Techniques \nfor crafting clear, concise, and informative prompts, Exploring advanced prompt \nengineering strategies (zero-shot learning, few-shot learning) , and case studies: \nsuccessful applications of prompt engineering. \n6.3 LLM architecture (transformers), understanding pre-training and fine-tuning of \nLLM, Popular LLM examples (GPT-3),Exploring LLM capabilities: text generation, \ntranslation, question answering, code generation  etc ,Langchain, Setting up \nEnvironment LangChain and LLM, Meta Llama2 , Google PaLM2 LLM"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of days to complete the syllabus (or press Enter to skip):\n",
            "6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Summary of the syllabus:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "هتي نصاب جو خلاصو آهي:\n\n**ماڊل مواد:**\n\n1. **اين ايل پي جو تعارف**\n   - NLP جو اصل ۽ تاريخ\n   - ٻولي پروسيسنگ ۾ ٻولي، علم، ۽ گرامر\n   - اين ايل پي ۾ اسٽيج، ابهام ۽ ان جا قسم\n   - اين ايل پي جون درخواستون\n\n2. **لفظ جي سطح جو تجزيو**\n   - ٽوڪنائيزيشن، اسٽيمنگ، ۽ لميٽائيزيشن\n   - مورفولوجي ماڊلز: ڊڪشنري لوڪ اپ ۽ فائنٽ اسٽيٽ مورفولوجي\n   - N-grams (Bigram ۽ Trigram) ۽ Smoothing ٽيڪنڪ\n\n3. **نحو جو تجزيو**\n   - پارٽ-آف-اسپيچ ٽيگنگ (POS) ۽ ان جا چئلينج\n   - ضابطي جي بنياد تي، اسٽوچسٽڪ، ۽ تبديلي جي بنياد تي ٽيگنگ\n   - لڪيل مارڪوف ماڊل (HMM) ۽ مشروط رينڊم فيلڊ (CRF)\n\n4. ** معنوي تجزيو**\n   - ليڪسيڪل سيمينٽڪس ۽ ڪورپس مطالعي جو تعارف\n   - لفظ احساس تڪرار (WSD) ۽ علم تي ٻڌل انداز\n\n5. **عمليات**\n   - بحث ۽ حوالو قرارداد\n   - هم آهنگي تي نحوي ۽ اصطلاحي پابنديون\n\n6. **Generative AI ۽ وڏي ٻولي جا ماڊل**\n   - پيداوار جو تعارف AI ۽ ان جا قسم\n   - فوري انجنيئرنگ ۽ وڏي ٻولي ماڊلز (LLMs)\n   - ايل ايل ايم آرڪيٽيڪچر، پري ٽريننگ، ۽ فائن ٽيوننگ\n   - ايل ايل ايم صلاحيتن کي ڳولڻ ۽ ماحول کي ترتيب ڏيڻ."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Study Plan:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "** ڏينهن 1: اين ايل پي جو تعارف ۽ لفظ سطح جي تجزيي**\n\n- صبح جو (9:00 AM - 12:00 PM): ٻولي پروسيسنگ ۾ NLP، ٻولي، علم ۽ گرامر جي اصليت ۽ تاريخ جو مطالعو ڪريو. (2 ڪلاڪ)\n  - ذريعو: \"Introduction to Natural Language Processing (NLP)\" Stanford University by Coursera\n  - وسيلو: \"قدرتي ٻوليءَ جي پروسيسنگ لاءِ هڪ نئون نمونو\" جان سووا طرفان (ڪتاب)\n- دوپہر (1:00 PM - 3:00 PM): اين ايل پي جي مرحلن بابت ڄاڻو، مونجهارو، ۽ انگريزي ۽ هندستاني علائقائي ٻولين ۾ ان جي قسمن. (2 ڪلاڪ)\n  - ذريعو: \"قدرتي ٻولي پروسيسنگ (تقريبا) شروع کان\" Collobert et al. (ڪتاب)\n  - ذريعو: \"قدرتي ٻولي پروسيسنگ ۾ ابهام\" ڊيوڊ يارووسڪي طرفان (تحقيق پيپر)\n- شام (3:00 PM - 5:00 PM): NLP ۽ ان جي ايپليڪيشنن جي چئلينجن جو مطالعو ڪريو. (2 ڪلاڪ)\n  - ذريعو: \"قدرتي ٻوليءَ جي پروسيسنگ ۾ چئلينجز\" جورافسڪي ۽ مارٽن پاران (ڪتاب)\n  - وسيلو: \"ايپليڪيشن آف نيچرل لينگويج پروسيسنگ\" IBM پاران (آرٽيڪل)\n\n** ڏينهن 2: لفظ جي سطح جو تجزيو (حصو 1)**\n\n- صبح جو (9:00 AM - 12:00 PM): بنيادي اصطلاحن جو مطالعو ڪريو: ٽوڪنائيزيشن، اسٽيمنگ، لميٽائيزيشن، ۽ سروي آف انگلش مورفولوجي. (3 ڪلاڪ)\n  - وسيلو: \"Tokenization ۽ Lemmatization\" NLTK پاران (ٽيوٽوريل)\n  - ذريعو: \"انگريزي مورفولوجي\" OUP پاران (ڪتاب)\n- منجھند جو (1:00 PM - 3:00 PM): انفليڪشنل ۽ ڊيريويشنل مورفولوجي بابت ڄاڻو. (2 ڪلاڪ)\n  - وسيلو: لسانيات پاران \"انفريڪشنل ۽ ڊيريوشنل مورفولوجي\" (آرٽيڪل)\n  - ذريعو: \"انگلش جو مورفولوجي\" ڪيمبرج يونيورسٽي پريس پاران (ڪتاب)\n- شام (3:00 PM - 5:00 PM): مطالعو مورفولوجيڪل ماڊل: ڊڪشنري لوڪ اپ ۽ محدود رياستي مورفولوجي. (2 ڪلاڪ)\n  - ذريعو: \"فائنٽ اسٽيٽ مورفولوجي\" اسٽينفورڊ يونيورسٽي طرفان (ٽيوٽوريل)\n  - وسيلو: \"لغت ڏس\" NLTK پاران (ٽيوٽوريل)\n\n**ڏينهن 3: لفظ جي سطح جو تجزيو (حصو 2)**\n\n- صبح جو (9:00 AM - 12:00 PM): FST (Finite State Transducer) سان morphological parsing بابت ڄاڻو. (3 ڪلاڪ)\n  - ذريعو: \"FST سان مورفولوجي پارسنگ\" اسٽينفورڊ يونيورسٽي طرفان (ٽيوٽوريل)\n  - ذريعو: \"فائنٽ اسٽيٽ ٽرانسڊيوسرز\" پاران آٽوماٽا (آرٽيڪل)\n- منجھند جو (1:00 PM - 3:00 PM): مطالعو گرام ۽ ان جي تبديلي: بگگرام، ٽرگرام، ۽ سادو (اڻ سموٿ ٿيل) اين-گرام. (2 ڪلاڪ)\n  - ذريعو: NLTK پاران \"N-Grams\" (ٽيوٽوريل)\n  - وسيلو: وڪيپيڊيا طرفان \"بيگرام ۽ ٽرگرام\" (آرٽيڪل)\n- شام (3:00 PM - 5:00 PM): ٽريننگ ڪورپس ۽ نامعلوم لفظن کي N-gram حساسيت بابت ڄاڻو: کليل بمقابله بند لفظي ڪم. (2 ڪلاڪ)\n  - ذريعو: اسٽينفورڊ يونيورسٽي طرفان \"اين-گرام حساسيت\" (ٽيوٽوريل)\n  - ذريعو: \"کليل ۽ بند لفظي ڪم\" لسانيات طرفان (آرٽيڪل)\n\n**ڏينهن 4: نحو جو تجزيو**\n\n- صبح جو (9:00 AM - 12:00 PM): انگريزي (Upenn Treebank) لاءِ پارٽ آف اسپيچ ٽيگنگ (POS) ۽ ٽيگ سيٽ جو مطالعو ڪريو. (3 ڪلاڪ)\n  - وسيلو: NLTK پاران \"پارٽ آف اسپيچ ٽيگنگ\" (ٽيوٽوريل)\n  - ذريعو: \"Penn Treebank\" UPenn پاران (آرٽيڪل)\n- منجھند جو (1:00 PM - 3:00 PM): POS ٽيگنگ ۾ مشڪلاتن ۽ چئلينجن بابت ڄاڻو. (2 ڪلاڪ)\n  - وسيلو: جورفسڪي پاران \"پارٽ-آف-اسپيچ ٽيگنگ ۾ چيلنجز\" ۽ مارٽن (ڪتاب)\n  - ذريعو: \"POS ٽيگنگ ۾ مشڪلات\" لسانيات طرفان (آرٽيڪل)\n- شام (3:00 PM - 5:00 PM): قاعدي تي ٻڌل، اسٽوچسٽڪ، ۽ ٽرانسفارميشن تي ٻڌل ٽيگنگ جو مطالعو ڪريو. (2 ڪلاڪ)\n  - ذريعو: اسٽينفورڊ يونيورسٽي طرفان \"قاعدي تي ٻڌل ٽيگنگ\" (ٽيوٽوريل)\n  - وسيلو: NLTK پاران \"اسٽڪاسٽڪ ۽ ٽرانسفارميشن تي ٻڌل ٽيگنگ\" (ٽيوٽوريل)\n\n**ڏينهن 5: نحوي تجزيا ۽ لفظي تجزيا**\n\n- صبح (9:00 AM - 12:00 PM): پيدا ٿيندڙ ماڊل بابت ڄاڻو: پوشیدہ مارڪوف ماڊل (HMM) ۽ ويٽربي POS ٽيگنگ لاءِ. (3 ڪلاڪ)\n  - ذريعو: \"پوشيده مارڪوف ماڊل\" اسٽينفورڊ يونيورسٽي طرفان (ٽيوٽوريل)\n  - ذريعو: \"Viterbi Algorithm\" وڪيپيڊيا طرفان (آرٽيڪل)\n- دوپہر (1:00 PM - 3:00 PM): مطالعي جو امتيازي ماڊل: وڌ ۾ وڌ اينٽراپي ماڊل ۽ مشروط رينڊم فيلڊ (CRF). (2 ڪلاڪ)\n  - ذريعو: \"وڌ کان وڌ اينٽروپي ماڊل\" اسٽينفورڊ يونيورسٽي طرفان (ٽيوٽوريل)\n  - وسيلو: \"شرطي بي ترتيب واري فيلڊ\" NLTK پاران (ٽيوٽوريل)\n- شام (شام 3:00 PM - 5:00 PM): مطالعو لفظي تجزيو: تعارف، مطلب نمائندگي، ۽ لغوي معني. (2 ڪلاڪ)\n  - ذريعو: \"Semantic Analysis\" Stanford University (tutorial)\n  - ذريعو: لسانيات طرفان \"ليڪسيڪل سيمينٽڪس\" (آرٽيڪل)\n\n**ڏينهن 6: معنوي تجزيو ۽ پيدا ڪندڙ AI**\n\n- صبح جو (9:00 AM - 12:00 PM): ڪورپس جي مطالعي ۽ مختلف ٻولين جي لغتن جي مطالعي بابت ڄاڻو جيئن WordNet. (3 ڪلاڪ)\n  - ذريعو: اسٽينفورڊ يونيورسٽي طرفان \"ڪورپس مطالعو\" (ٽيوٽوريل)\n  - ذريعو: \"WordNet\" پاران پرنسٽن يونيورسٽي (آرٽيڪل)\n- دوپہر (1:00 PM - 3:00 PM): ليڪسيمز ۽ انهن جي حواس جي وچ ۾ لاڳاپن جو مطالعو ڪريو: homonymy, polysemy, synonymy, and hyponymy. (2 ڪلاڪ)\n  - وسيلو: لسانيات طرفان \"ليڪسيڪل رابطا\" (آرٽيڪل)\n  - ذريعو: NLTK پاران \"لفظ احساس تڪرار\" (ٽيوٽوريل)\n- شام (3:00 PM - 5:00 PM): جنريٽو AI جو مطالعو ڪريو: تعارف، جنريٽيو AI ماڊل جا قسم، ۽ فائدا ۽ حدون. (2 ڪلاڪ)\n  - ذريعو: اسٽينفورڊ يونيورسٽي طرفان \"جنريٽو اي آئي\" (ٽيوٽوريل)\n  - وسيلو: وڪيپيڊيا پاران \"پيداوار AI ماڊلز\" (آرٽيڪل)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Key Topics"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Generative AI، Meta Llama2، Google PaLM2 LLM، سروي آف انگلش مورفولوجي، اسٽوچسٽڪ ۽ ٽرانسفارميشن، انفليڪشنل مورفولوجي، مشڪلاتون، سيمينٽڪ ايناليسس، وڏي ٻوليءَ جا ماڊل، ٽيمپليٽس، ٽيگ، ايل ايل ايم، ڊيريوشنل مورفولوجي، بگرام، گرامر، اين اي گرامر , Langchain, LangChain, Laplace Smoothing, Language, FST, Algorithm, Prompt Engineering, Upenn Treebank, Simple, Lexical Semantics, Generative Adversarial Networks, Module Content, HMM, Introduction  \nاصل ۽ تاريخ، مارڪوف ماڊل، اين-\nگرام حساسيت، نحوي ۽ اصطلاحي، NLP جا چئلينج، Synonymy، NLP، Generative AI، Word Sense Disambiguation (WSD)؛ علم، Anaphora، WorldNet، Parsers"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        }
      ]
    }
  ]
}